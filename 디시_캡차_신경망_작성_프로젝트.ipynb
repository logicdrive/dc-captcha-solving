{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ZoyamfVNMeHe",
        "IwPquY_HMiiU",
        "7EltMmAyMqSb",
        "628kDkSfMuWk",
        "oeiKZ4ukMzvR",
        "4vrxbaGwM7Rz"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### 1. 환경 세팅하기\n",
        "-----"
      ],
      "metadata": {
        "id": "ZoyamfVNMeHe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-H700n2FMVPq"
      },
      "outputs": [],
      "source": [
        "# 관련 모듈들을 다운로드 받고, 주요 폴더를 미리 생성시키기 위해서\n",
        "!pip install pytorch-lightning torchmetrics plotly gradio opencv-python\n",
        "!mkdir ./check_points ./lightning_logs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습을 위해서 업로드된 압축파일을 풀기 위해서\n",
        "!unzip ./Captcha_Train_Set_7000.zip\n",
        "!rm ./Captcha_Train_Set_7000.zip"
      ],
      "metadata": {
        "id": "bv96v247Mc03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. 학습 및 평가를 위한 데이터 로더 생성시키기\n",
        "-----"
      ],
      "metadata": {
        "id": "IwPquY_HMiiU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import glob\n",
        "import numpy as np\n",
        "\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from PIL import Image\n",
        "from torchvision.transforms import Compose, ToTensor, Normalize\n",
        "\n",
        "# 주어진 이미지 파일 경로의 이미지들을 이용해서 캡차 데이터셋을 생성시키기 위해서\n",
        "class Captcha_Dataset(Dataset) :\n",
        "  def __init__(self, image_dir_path) :\n",
        "    self.CORPUS = string.ascii_lowercase + string.digits\n",
        "    self.BOW = self.__make_BOW(self.CORPUS)\n",
        "    self.IMAGE_FILE_PATHS = glob.glob(image_dir_path + \"/*.jpg\")\n",
        "    self.COMPOSE = Compose([\n",
        "      ToTensor(),\n",
        "      Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
        "    ])\n",
        "\n",
        "  def __len__(self) :\n",
        "    return len(self.IMAGE_FILE_PATHS)\n",
        "\n",
        "  def __getitem__(self, i) :\n",
        "    IMAGE = self.COMPOSE(Image.open(self.IMAGE_FILE_PATHS[i]).convert(\"L\").convert(\"RGB\"))\n",
        "    DATA = np.array(IMAGE).astype(np.float32)\n",
        "    LABEL = np.array(self.__get_seq(self.__get_Label_From_Image_Path(self.IMAGE_FILE_PATHS[i])))\n",
        "    return DATA, LABEL\n",
        "\n",
        "  # 주어진 이미지 경로로부터 라벨을 추출시킨 결과를 반환하기 위해서\n",
        "  def __get_Label_From_Image_Path(self, image_path) :\n",
        "    return image_path.split(\"/\")[-1].split(\".\")[0]\n",
        "\n",
        "  # 주어진 문자열들을 BOW를 이용해서 정수 리스트로 변환시키기 위해서\n",
        "  def __get_seq(self, letters) :\n",
        "    return list(map(lambda letter : self.BOW[letter], letters))\n",
        "\n",
        "  # 주어진 문자열들에 대한 BOW를 생성시키고 반환하기 위해서\n",
        "  def __make_BOW(self, corpus) :\n",
        "    bow = {\"<pad>\":0}\n",
        "\n",
        "    for letter in corpus :\n",
        "      if letter not in bow :\n",
        "        bow[letter] = len(bow)\n",
        "\n",
        "    return bow"
      ],
      "metadata": {
        "id": "XtidVTSMMln8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습, 검증, 테스트 데이터셋 불러오고 관련 데이터 로더를 생성시키기 위해서\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "\n",
        "TRAIN_DATASET = Captcha_Dataset(\"./Captcha_Train_Set_7000/train_data\")\n",
        "VALID_DATASET = Captcha_Dataset(\"./Captcha_Train_Set_7000/valid_data\")\n",
        "TEST_DATASET = Captcha_Dataset(\"./Captcha_Train_Set_7000/test_data\")\n",
        "\n",
        "TRAIN_LOADER = DataLoader(TRAIN_DATASET, batch_size=8, shuffle=True, drop_last=True)\n",
        "VALID_LOADER = DataLoader(VALID_DATASET, batch_size=8, shuffle=False, drop_last=True)\n",
        "TEST_LOADER = DataLoader(TEST_DATASET, batch_size=8, shuffle=False, drop_last=True)"
      ],
      "metadata": {
        "id": "NQkQ7ADfMnIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. 신경망 모델 생성하기\n",
        "-----"
      ],
      "metadata": {
        "id": "7EltMmAyMqSb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pytorch_lightning as pl\n",
        "import torchmetrics\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.adam import Adam\n",
        "\n",
        "\n",
        "# 이미지 크기를 줄이면서 ResNet 구조를 사용하기 위한 기본 모듈\n",
        "class Basic_ResNet_Downsample_Layer(nn.Module) :\n",
        "  def __init__(self, in_channels, out_channels) :\n",
        "    super().__init__()\n",
        "    \n",
        "    self.CONV_1 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=(3, 5), stride=(2, 1))\n",
        "    self.CONV_2 = nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=(3, 3), padding=1)\n",
        "\n",
        "    self.BN_1 = nn.BatchNorm2d(num_features=out_channels)\n",
        "    self.BN_2 = nn.BatchNorm2d(num_features=out_channels)\n",
        "\n",
        "    self.LERU = nn.ReLU()\n",
        "\n",
        "    self.CONV_DOWN_SAMPLE = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=(3, 5), stride=(2, 1))\n",
        "\n",
        "    self.LAYER_SEQ = nn.Sequential(\n",
        "      self.CONV_1, self.BN_1, self.LERU,\n",
        "      self.CONV_2, self.BN_2\n",
        "    )\n",
        "  \n",
        "  def forward(self, x) :\n",
        "    x = self.LAYER_SEQ(x) + self.CONV_DOWN_SAMPLE(x)\n",
        "    x = self.LERU(x)\n",
        "    return x\n",
        "\n",
        "# 이미지 크기를 유지하면서 ResNet 구조를 사용하기 위한 기본 모듈\n",
        "class Basic_ResNet_Maintain_Layer(nn.Module) :\n",
        "  def __init__(self, in_channels, out_channels) :\n",
        "    super().__init__()\n",
        "    \n",
        "    self.CONV = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=(3, 3), padding=1)\n",
        "    self.BN = nn.BatchNorm2d(num_features=out_channels)\n",
        "    self.LERU = nn.ReLU()\n",
        "\n",
        "    self.LAYER_SEQ = nn.Sequential(\n",
        "      self.CONV, self.BN, self.LERU\n",
        "    )\n",
        "  \n",
        "  def forward(self, x) :\n",
        "    return self.LAYER_SEQ(x) + x\n",
        "\n",
        "\n",
        "# 셀프 어텐션을 사용하는 Skip Layer 구조를 사용하기 위한 기본 모둘\n",
        "class Basic_Self_Attension_Skip_Layer(nn.Module) :\n",
        "  def __init__(self, input_size) :\n",
        "    super().__init__()\n",
        "\n",
        "    self.ATTENTION = torch.nn.MultiheadAttention(input_size, 8)\n",
        "    self.BN = nn.BatchNorm1d(num_features=8)\n",
        "  \n",
        "  def forward(self, x) :\n",
        "    x_ = x\n",
        "    x, _ = self.ATTENTION(x, x, x)\n",
        "    x = self.BN(x)\n",
        "    return x + x_\n",
        "\n",
        "\n",
        "# 기본 모듈을 겹겹히 조합하기 위한 주요 신경망 모듈\n",
        "class CRNN_Module(pl.LightningModule) :\n",
        "  def __init__(self, bow) :\n",
        "    super().__init__()\n",
        "    self.BOW = bow\n",
        "    self.REV_BOW = list(self.BOW.keys())\n",
        "\n",
        "    resnet_layers = []\n",
        "    resnet_layers.append(Basic_ResNet_Downsample_Layer(in_channels=3, out_channels=32))\n",
        "    for layer_index in range(1, 47+1) :\n",
        "      if layer_index%12 == 0 : resnet_layers.append(Basic_ResNet_Downsample_Layer(in_channels=32, out_channels=32))\n",
        "      else : resnet_layers.append(Basic_ResNet_Maintain_Layer(in_channels=32, out_channels=32))\n",
        "    resnet_layers.append(nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(2, 5)))\n",
        "    self.RESNET_LAYER_SEQ = nn.Sequential(*resnet_layers)\n",
        "\n",
        "\n",
        "    attenion_layers = [Basic_Self_Attension_Skip_Layer(32) for _ in range(5)]\n",
        "    self.SELF_ATTENSION_SKIP_LAYER_SEQ = nn.Sequential(*attenion_layers)\n",
        "\n",
        "\n",
        "    self.FC_1 = nn.Linear(32, 64)\n",
        "    self.FC_2 = nn.Linear(64, len(self.BOW))\n",
        "    self.LERU = nn.ReLU()\n",
        "\n",
        "    self.FC_LAYER_SEQ = nn.Sequential(\n",
        "        self.FC_1, self.LERU, self.FC_2\n",
        "    )\n",
        "  \n",
        "  def forward(self, x) :\n",
        "    x = self.RESNET_LAYER_SEQ(x)\n",
        "    x = x.view(x.shape[0], 32, -1)\n",
        "    x = x.permute(2, 0, 1)\n",
        "\n",
        "    x = self.SELF_ATTENSION_SKIP_LAYER_SEQ(x)\n",
        "    \n",
        "    x = self.FC_LAYER_SEQ(x)\n",
        "\n",
        "    x = F.log_softmax(x, dim=-1)\n",
        "    return x\n",
        "\n",
        "\n",
        "  def training_step(self, batch, batch_idx) :\n",
        "    CTC_LOSS = self.__forward_To_CTC_Loss(batch)\n",
        "    self.log('train_ctc_loss', CTC_LOSS, on_epoch=True, prog_bar=True)\n",
        "    self.log('train_accuracy', self.__accuracy(batch), on_epoch=True, prog_bar=True)\n",
        "    return CTC_LOSS\n",
        "  \n",
        "  def validation_step(self, batch, batch_idx) :\n",
        "    self.log('valid_accuracy', self.__accuracy(batch), on_epoch=True, prog_bar=True)\n",
        "  \n",
        "  def test_step(self, batch, batch_idx) :\n",
        "    self.log('test_accuracy', self.__accuracy(batch), on_epoch=True, prog_bar=True)\n",
        "  \n",
        "  def predict_step(self, batch, batch_idx) :\n",
        "    X, Y = batch\n",
        "    return self.__pred_To_Letters(X)\n",
        "\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "      return Adam(self.parameters(), lr=1e-4)\n",
        "\n",
        "\n",
        "  # 주어진 배치에 대한 CTC 손실을 반환시킥 위해서\n",
        "  def __forward_To_CTC_Loss(self, batch) :\n",
        "    X, Y = batch\n",
        "    PREDS = self(X)\n",
        "\n",
        "    PREDS_SIZE = torch.IntTensor([PREDS.size(0)]*PREDS.size(1))\n",
        "    TARGET_SIZE = torch.IntTensor([len(y_each) for y_each in Y])\n",
        "\n",
        "    CTC_LOSS = nn.CTCLoss(blank=0)(PREDS, Y, PREDS_SIZE, TARGET_SIZE)\n",
        "    return CTC_LOSS\n",
        "  \n",
        "  # 주어진 배치에 대한 정확도를 반환시키기 위해서\n",
        "  def __accuracy(self, batch) :\n",
        "    X, Y = batch\n",
        "    Y_LETTERS = self.__y_To_Letters(Y)\n",
        "    PRED_LETTERS = self.__pred_To_Letters(X)\n",
        "\n",
        "    correct_count = 0\n",
        "    for pred_index in range(len(PRED_LETTERS)) :\n",
        "      if PRED_LETTERS[pred_index] == Y_LETTERS[pred_index] : correct_count += 1\n",
        "    \n",
        "    ACCURACY = correct_count/len(PRED_LETTERS)\n",
        "    return ACCURACY\n",
        "  \n",
        "  # 신경망에서 예측된 백터를 문자열들로 반환시키기 위해서\n",
        "  def __pred_To_Letters(self, x) :\n",
        "    PREDS = self(x).transpose(1,0)\n",
        "    PRED_ARGMAXS = torch.argmax(PREDS, dim=-1)\n",
        "\n",
        "    output_pred_letters = []\n",
        "    for pred_argmax in PRED_ARGMAXS :\n",
        "      PRED_ARGMAX_PROCESSED = self.__process_Model_Predict(pred_argmax)\n",
        "      PRED_LETTERS = self.__predict_To_Letters(PRED_ARGMAX_PROCESSED)\n",
        "      output_pred_letters.append(PRED_LETTERS)\n",
        "    return output_pred_letters\n",
        "\n",
        "  # CTC 손실로 중복 예측된 라벨들의 중복을 제거시키고 정제하기 위해서\n",
        "  def __process_Model_Predict(self, pred_argmax) :\n",
        "    pred_letters = []\n",
        "    prev_letter = pred_argmax[0].item()\n",
        "    if prev_letter != 0 : pred_letters.append(prev_letter)\n",
        "\n",
        "    for letter in pred_argmax :\n",
        "      if letter.item() != 0 and letter.item() != prev_letter :\n",
        "        pred_letters.append(letter.item())\n",
        "      prev_letter = letter.item()\n",
        "    return pred_letters\n",
        "  \n",
        "  # 예측된 코드를 문자열로 변환시키기 위해서\n",
        "  def __predict_To_Letters(self, pred) :\n",
        "    return \"\".join([self.REV_BOW[code] for code in pred])\n",
        "  \n",
        "  # 정답 백터를 문자열 리스트로 변환시키기 위해서\n",
        "  def __y_To_Letters(self, y) :\n",
        "    return [\"\".join([self.REV_BOW[code] for code in y_each]) for y_each in y]"
      ],
      "metadata": {
        "id": "_8hdk0rkMrAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. 신경망 학습하기\n",
        "-----"
      ],
      "metadata": {
        "id": "628kDkSfMuWk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 진행상황을 모니터링하기 위해서\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir lightning_logs/"
      ],
      "metadata": {
        "id": "caObukr9Mu7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 신경망을 학습시키고, 체크포인트에 가장 정확도가 높은 모델을 저장시키기 위해서\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "CHECK_POINT_CALLBACK = ModelCheckpoint(dirpath=\"./check_points\", monitor=\"valid_accuracy\", mode=\"max\", filename=\"crnn-{epoch:02d}-{valid_accuracy:.2f}\")\n",
        "LOGGER = pl.loggers.TensorBoardLogger(name=f'CRNN_MODULE_LOGS', save_dir='lightning_logs')\n",
        "TRAINER = pl.Trainer(max_epochs=1000, accelerator=DEVICE, callbacks=[CHECK_POINT_CALLBACK], logger=LOGGER)\n",
        "\n",
        "\n",
        "CRNN_MODULE = CRNN_Module(bow=TRAIN_DATASET.BOW)\n",
        "TRAINER.fit(CRNN_MODULE, train_dataloaders=TRAIN_LOADER, val_dataloaders=VALID_LOADER)"
      ],
      "metadata": {
        "id": "RO5sTtYMMwKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. 모델 테스트하기\n",
        "-----"
      ],
      "metadata": {
        "id": "oeiKZ4ukMzvR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련된 모델에 대한 Train, Valid, Test 데이터 로더 관련 정확도를 출력시키기 위해서\n",
        "CRNN_MODULE_TEST = CRNN_Module.load_from_checkpoint(\"/content/Resnet_With_Attention_Complete_Train-0.9992_Valid-0.9841_Test-0.9870.ckpt\", bow=TRAIN_DATASET.BOW)\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "TRAINER = pl.Trainer(accelerator=DEVICE)\n",
        "\n",
        "TRAINER.test(CRNN_MODULE_TEST, dataloaders=TRAIN_LOADER)\n",
        "TRAINER.test(CRNN_MODULE_TEST, dataloaders=VALID_LOADER)\n",
        "TRAINER.test(CRNN_MODULE_TEST, dataloaders=TEST_LOADER)"
      ],
      "metadata": {
        "id": "zhymM0oaM3jj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. 모델을 사용하는 사용자 인터페이스 생성하기\n",
        "-----"
      ],
      "metadata": {
        "id": "4vrxbaGwM7Rz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "# 신경망을 통해서 캡차 데이터를 일관성있게 예측하기 위해서\n",
        "class Predict_Captcha_Label :\n",
        "  def __init__(self, model_path, bow) :\n",
        "    self.MODEL = CRNN_Module.load_from_checkpoint(model_path, bow=bow)\n",
        "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    self.TRAINER = pl.Trainer(accelerator=DEVICE, enable_progress_bar=False, logger=False)\n",
        "\n",
        "  def __preprocessing_Image(self, image) :\n",
        "    USER_IMAGE = Image.fromarray(np.uint8(image)).convert(\"L\").convert(\"RGB\")\n",
        "    USER_IMAGE = Compose([\n",
        "      ToTensor(),\n",
        "      Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
        "    ])(USER_IMAGE)\n",
        "    return USER_IMAGE\n",
        "\n",
        "  def __convert_Image_To_Data_Loader(self, images) :\n",
        "    user_images = list(map(self.__preprocessing_Image, images))\n",
        "    while len(user_images) != 8 :\n",
        "      user_images.append(user_images[0])\n",
        "    \n",
        "    USER_IMAGES = torch.stack(user_images)\n",
        "    USER_IMAGE_DATASET = TensorDataset(USER_IMAGES, torch.zeros(8))\n",
        "    USER_IMAGE_LOADER = DataLoader(USER_IMAGE_DATASET, batch_size=8)\n",
        "    return USER_IMAGE_LOADER\n",
        "  \n",
        "  # 단일 이미지에 대한 예측 결과를 반환시키기 위해서\n",
        "  def predict_Captcha_Label(self, image) :\n",
        "      DATA_LOADER = self.__convert_Image_To_Data_Loader([image])\n",
        "      PRED = self.TRAINER.predict(self.MODEL, dataloaders=DATA_LOADER)[0][0]\n",
        "      return PRED\n",
        "  \n",
        "  # 여러 이미지에 대한 예측 결과를 반환시키기 위해서\n",
        "  def predict_Captcha_Labels(self, images) :\n",
        "    DATA_LOADER = self.__convert_Image_To_Data_Loader(images)\n",
        "    PREDS = self.TRAINER.predict(self.MODEL, dataloaders=DATA_LOADER)[0][:len(images)]\n",
        "    return PREDS"
      ],
      "metadata": {
        "id": "nv9Qw4gnM73s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 이미지 분류를 인식시키기 위한 유저 인터페이스를 제공하기 위해서\n",
        "import gradio as gr\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "PREDICT_CAPTCHA_LABEL = Predict_Captcha_Label(\"/content/Resnet_With_Attention_Complete_Train-0.9992_Valid-0.9841_Test-0.9870.ckpt\", bow=TRAIN_DATASET.BOW)\n",
        "def cpatcha_Recognition(user_image):\n",
        "  return PREDICT_CAPTCHA_LABEL.predict_Captcha_Label(user_image)\n",
        "\n",
        "gr.Interface(fn=cpatcha_Recognition, inputs=\"image\", outputs=\"text\").launch(debug=True)"
      ],
      "metadata": {
        "id": "m0JeslGXM82n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}